import * as _llamaindex_core_agent from '@llamaindex/core/agent';
import { AgentWorker, TaskHandler, AgentRunner, AgentParamsBase } from '@llamaindex/core/agent';
export * from '@llamaindex/core/agent';
import * as _llamaindex_core_schema from '@llamaindex/core/schema';
import * as _llamaindex_core_chat_engine from '@llamaindex/core/chat-engine';
import * as _llamaindex_core_llms from '@llamaindex/core/llms';
import { LLM, ChatResponse } from '@llamaindex/core/llms';
import * as _llamaindex_core_retriever from '@llamaindex/core/retriever';
import { BaseRetriever } from '@llamaindex/core/retriever';
import { OpenAIAgentParams } from '@llamaindex/openai';
import { JSONValue, JSONObject } from '@llamaindex/core/global';

interface ContextAwareConfig {
    contextRetriever: BaseRetriever;
}
type AgentParams = OpenAIAgentParams;

declare const OpenAIContextAwareAgent: {
    new (params: AgentParams & ContextAwareConfig): {
        readonly contextRetriever: _llamaindex_core_retriever.BaseRetriever;
        retrievedContext: string | null;
        retrieveContext(query: _llamaindex_core_llms.MessageContent): Promise<string>;
        injectContext(context: string): Promise<void>;
        chat(params: _llamaindex_core_chat_engine.NonStreamingChatEngineParams): Promise<_llamaindex_core_schema.EngineResponse>;
        chat(params: _llamaindex_core_chat_engine.StreamingChatEngineParams): Promise<ReadableStream<_llamaindex_core_schema.EngineResponse>>;
        createStore: typeof _llamaindex_core_agent.AgentRunner.defaultCreateStore;
        taskHandler: _llamaindex_core_agent.TaskHandler<_llamaindex_core_llms.LLM<object, object>>;
        "__#13@#private": any;
        readonly llm: _llamaindex_core_llms.LLM<object, object>;
        readonly chatHistory: _llamaindex_core_llms.ChatMessage<object>[];
        readonly verbose: boolean;
        reset(): void;
        getTools(query: _llamaindex_core_llms.MessageContent): Promise<_llamaindex_core_llms.BaseToolWithCall[]> | _llamaindex_core_llms.BaseToolWithCall[];
        createTask(message: _llamaindex_core_llms.MessageContent, stream?: boolean, verbose?: boolean | undefined, chatHistory?: _llamaindex_core_llms.ChatMessage<object>[] | undefined, additionalChatOptions?: object | undefined): ReadableStream<{
            taskStep: _llamaindex_core_agent.TaskStep<_llamaindex_core_llms.LLM<object, object>, object, object, object>;
            output: _llamaindex_core_llms.ChatResponse<object> | ReadableStream<_llamaindex_core_llms.ChatResponseChunk<object>>;
            isLast: boolean;
        }>;
    };
    defaultCreateStore(): object;
    defaultTaskHandler: _llamaindex_core_agent.TaskHandler<_llamaindex_core_llms.LLM>;
    shouldContinue<AI extends _llamaindex_core_llms.LLM, Store extends object = object, AdditionalMessageOptions extends object = AI extends _llamaindex_core_llms.LLM<object, infer AdditionalMessageOptions_1 extends object> ? AdditionalMessageOptions_1 : never>(task: Readonly<_llamaindex_core_agent.TaskStep<AI, Store, AdditionalMessageOptions>>): boolean;
};

type ReACTAgentParams = AgentParamsBase<LLM>;
type BaseReason = {
    type: unknown;
};
type ObservationReason = BaseReason & {
    type: "observation";
    observation: JSONValue;
};
type ActionReason = BaseReason & {
    type: "action";
    thought: string;
    action: string;
    input: JSONObject;
};
type ResponseReason = BaseReason & {
    type: "response";
    thought: string;
    response: ChatResponse;
};
type Reason = ObservationReason | ActionReason | ResponseReason;
type ReACTAgentStore = {
    reasons: Reason[];
};
declare class ReACTAgentWorker extends AgentWorker<LLM, ReACTAgentStore> {
    taskHandler: TaskHandler<LLM<object, object>, ReACTAgentStore>;
}
declare class ReActAgent extends AgentRunner<LLM, ReACTAgentStore> {
    constructor(params: ReACTAgentParams);
    createStore(): {
        reasons: never[];
    };
    static taskHandler: TaskHandler<LLM, ReACTAgentStore>;
}

export { OpenAIContextAwareAgent, type ReACTAgentParams, ReACTAgentWorker, ReActAgent };
